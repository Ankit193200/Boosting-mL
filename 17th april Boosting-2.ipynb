{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a513eaca-63ec-411d-88a2-9a14cbadec8e",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "**Gradient Boosting Regression** is a machine learning algorithm used for regression tasks. It belongs to the family of ensemble methods and builds a predictive model in the form of a series of weak learners, usually decision trees. The model is trained sequentially, with each new weak learner correcting the errors made by the combined ensemble of learners trained so far. Gradient Boosting Regression optimizes a loss function (usually the mean squared error) by iteratively fitting new models to the negative gradient of the loss function.\n",
    "\n",
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy.\n",
    "\n",
    "Here is a basic implementation of a gradient boosting algorithm for regression using Python and NumPy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a sample dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gradient Boosting algorithm\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        prediction = np.zeros_like(y)\n",
    "        for _ in range(self.n_estimators):\n",
    "            residual = y - prediction\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "            update = tree.predict(X).reshape(-1, 1)\n",
    "            prediction += self.learning_rate * update\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return sum(self.learning_rate * model.predict(X).reshape(-1, 1) for model in self.models)\n",
    "\n",
    "# Train the gradient boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "```\n",
    "\n",
    "### Q3. Experiment with different hyperparameters to optimize the performance of the model.\n",
    "\n",
    "You can experiment with hyperparameters using grid search or random search. Here's an example using scikit-learn's `GridSearchCV`:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create GradientBoostingRegressor\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "best_gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_best = best_gb_model.predict(X_test)\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "print(\"Best Model Mean Squared Error:\", mse_best)\n",
    "print(\"Best Model R-squared:\", r2_best)\n",
    "```\n",
    "\n",
    "### Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "In Gradient Boosting, a **weak learner** is a model that performs slightly better than random chance. Typically, decision trees with limited depth are used as weak learners. These trees are often referred to as \"stumps\" when they have only one node (split) or a small number of nodes.\n",
    "\n",
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind Gradient Boosting is to combine multiple weak learners to create a strong learner iteratively. The algorithm minimizes the errors made by the existing ensemble by fitting new models to the negative gradient of the loss function. Each new model corrects the mistakes of the combined ensemble, gradually improving the predictive performance.\n",
    "\n",
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Gradient Boosting builds an ensemble of weak learners through a sequential process. At each iteration:\n",
    "1. A new weak learner is trained to correct the errors made by the existing ensemble.\n",
    "2. The predictions of the weak learner are multiplied by a learning rate and added to the ensemble.\n",
    "3. The process is repeated for a specified number of iterations (number of trees).\n",
    "\n",
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "**Mathematical intuition of Gradient Boosting:**\n",
    "1. **Initialize the model:** Start with a simple model, often the mean of the target variable.\n",
    "2. **Compute the residuals:** Calculate the difference between the actual values and the predictions of the current model.\n",
    "3. **Fit a weak learner to the residuals:** Train a weak learner (e.g., decision tree) to predict the residuals.\n",
    "4. **Update the model:** Add the predictions of the weak learner (scaled by a learning rate) to the current model.\n",
    "5. **Repeat steps 2-4:** Continue the process until a predefined number of weak learners are trained.\n",
    "6. **Combine weak learners:** The final model is the sum of all weak learners.\n",
    "\n",
    "The algorithm minimizes a specified loss function by updating the predictions of the ensemble in the direction of the negative gradient of the loss with respect to the predictions. This ensures that each new weak learner corrects the mistakes of the existing ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68afa02e-4a97-478d-af34-7a562769341e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
